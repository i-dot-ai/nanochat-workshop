#!/bin/bash
set -e

# =============================================================================
# NANOCHAT WORKSHOP DEMO - Full 4-Stage Pipeline
# =============================================================================
# Module 3: Training Pipeline End-to-End
#
# TIMING BREAKDOWN (tested on MacBook M3):
#   Stage 1: Base Training (30K steps)  ~13 min
#   Stage 2: Mid Training (1000 steps)   ~4 min
#   Stage 3: SFT (200 steps)             ~2 min
#   Stage 4: RL (30 steps)              ~11 min  [optional]
#   ------------------------------------------
#   TOTAL:                              ~30 min
#
# Run as: bash workshop/03_training/workshop_demo.sh
# =============================================================================

echo ""
echo "  â–ˆâ–ˆâ–ˆ       â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ"
echo " â–’â–’â–’       â–ˆâ–ˆâ–ˆâ–’â–’â–’â–’â–’â–ˆâ–ˆâ–ˆ â–’â–’â–ˆâ–ˆâ–ˆ "
echo " â–ˆâ–ˆâ–ˆâ–ˆ     â–’â–ˆâ–ˆâ–ˆ    â–’â–ˆâ–ˆâ–ˆ  â–’â–ˆâ–ˆâ–ˆ "
echo "â–’â–’â–ˆâ–ˆâ–ˆ     â–’â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  â–’â–ˆâ–ˆâ–ˆ "
echo " â–’â–ˆâ–ˆâ–ˆ     â–’â–ˆâ–ˆâ–ˆâ–’â–’â–’â–’â–’â–ˆâ–ˆâ–ˆ  â–’â–ˆâ–ˆâ–ˆ "
echo " â–’â–ˆâ–ˆâ–ˆ     â–’â–ˆâ–ˆâ–ˆ    â–’â–ˆâ–ˆâ–ˆ  â–’â–ˆâ–ˆâ–ˆ "
echo " â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â–ˆâ–ˆ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ"
echo "â–’â–’â–’â–’â–’ â–’â–’ â–’â–’â–’â–’â–’   â–’â–’â–’â–’â–’ â–’â–’â–’â–’â–’ "
echo ""
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo "  ğŸ¤– NANOCHAT WORKSHOP - Train Your Own ChatGPT"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo ""
echo "  ğŸ“‹ Pipeline: Setup â†’ Base â†’ Mid â†’ SFT â†’ RL"
echo "  â±ï¸  Total time: ~30 minutes (on MacBook M3 ğŸ¤)"
echo ""

# Environment setup
export OMP_NUM_THREADS=1
export NANOCHAT_BASE_DIR="$HOME/.cache/nanochat"
export PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0
mkdir -p "$NANOCHAT_BASE_DIR"

# Activate virtual environment
if [ ! -d ".venv" ]; then
    echo "ğŸ“¦ Creating virtual environment..."
    uv venv
    uv sync --extra cpu
fi
source .venv/bin/activate

MODEL_TAG="${1:-workshop}"
echo "ğŸ·ï¸  Model tag: $MODEL_TAG"
echo ""

# =============================================================================
# STAGE 0: Setup (Tokenizer + Data)
# =============================================================================
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“"
echo "â”ƒ  ğŸ”¤ STAGE 0: Setup                             â”ƒ"
echo "â”ƒ  Downloading tokenizer and training data       â”ƒ"
echo "â”—â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”›"
echo ""

# Download tokenizer from HuggingFace
python -c "
import os
import shutil

tokenizer_dir = os.path.expanduser('~/.cache/nanochat/tokenizer')
os.makedirs(tokenizer_dir, exist_ok=True)

tokenizer_path = os.path.join(tokenizer_dir, 'tokenizer.pkl')
token_bytes_path = os.path.join(tokenizer_dir, 'token_bytes.pt')

if os.path.exists(tokenizer_path) and os.path.exists(token_bytes_path):
    print('  âœ… Tokenizer already exists')
else:
    from huggingface_hub import hf_hub_download
    print('  ğŸ“¥ Downloading tokenizer from karpathy/nanochat-d32...')
    tok_file = hf_hub_download('karpathy/nanochat-d32', 'tokenizer.pkl')
    bytes_file = hf_hub_download('karpathy/nanochat-d32', 'token_bytes.pt')
    shutil.copy(tok_file, tokenizer_path)
    shutil.copy(bytes_file, token_bytes_path)
    print(f'  âœ… Tokenizer installed')
"

# Download training data (2 shards = ~500MB, enough for workshop)
echo "  ğŸ“¥ Downloading training data (FineWeb-Edu)..."
python -m nanochat.dataset -n 2

# Download identity conversations (used in midtraining)
IDENTITY_FILE="$NANOCHAT_BASE_DIR/identity_conversations.jsonl"
if [ ! -f "$IDENTITY_FILE" ]; then
    echo "  ğŸ“¥ Downloading identity conversations..."
    curl -sL -o "$IDENTITY_FILE" https://karpathy-public.s3.us-west-2.amazonaws.com/identity_conversations.jsonl
    echo "  âœ… Identity conversations downloaded"
fi
echo ""

# =============================================================================
# STAGE 1: Base Training
# =============================================================================
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“"
echo "â”ƒ  ğŸ“š STAGE 1: Base Training                     â”ƒ"
echo "â”ƒ  Learning language from raw text               â”ƒ"
echo "â”£â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”«"
echo "â”ƒ  Steps: 30,000  |  Expected: ~13 min           â”ƒ"
echo "â”—â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”›"
echo ""
START=$(date +%s)

# Filter output: show every 300th step (1% progress), pass through all other lines
python -m scripts.base_train \
    --depth=4 \
    --max_seq_len=512 \
    --device_batch_size=1 \
    --total_batch_size=512 \
    --num_iterations=30000 \
    --eval_every=10000 \
    --eval_tokens=65536 \
    --core_metric_every=-1 \
    --sample_every=30000 \
    --model_tag="$MODEL_TAG" 2>&1 | awk '/^step [0-9]/ { n=substr($2,1,5)+0; if(n%300==0) print; next } {print}'

END=$(date +%s)
echo ""
echo "âœ… Base training complete! ($((END-START))s)"
echo ""

# =============================================================================
# STAGE 2: Mid Training
# =============================================================================
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“"
echo "â”ƒ  ğŸ”§ STAGE 2: Mid Training                      â”ƒ"
echo "â”ƒ  Learning conversation format + tools          â”ƒ"
echo "â”£â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”«"
echo "â”ƒ  Steps: 1,000   |  Expected: ~4 min            â”ƒ"
echo "â”—â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”›"
echo ""
START=$(date +%s)

# Filter output: show every 250th step line, pass through all other lines
python -m scripts.mid_train \
    --max_seq_len=1024 \
    --device_batch_size=1 \
    --total_batch_size=1024 \
    --num_iterations=1000 \
    --eval_every=500 \
    --eval_tokens=32768 \
    --model_tag="$MODEL_TAG" 2>&1 | awk '/^step [0-9]/ { n=substr($2,1,5)+0; if(n%250==0) print; next } {print}'

END=$(date +%s)
echo ""
echo "âœ… Mid training complete! ($((END-START))s)"
echo ""

# =============================================================================
# STAGE 3: SFT
# =============================================================================
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“"
echo "â”ƒ  ğŸ’¬ STAGE 3: Supervised Fine-Tuning            â”ƒ"
echo "â”ƒ  Learning to be a helpful assistant            â”ƒ"
echo "â”£â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”«"
echo "â”ƒ  Steps: 200     |  Expected: ~2 min            â”ƒ"
echo "â”—â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”›"
echo ""
START=$(date +%s)

# Filter output: show every 50th step line, pass through all other lines
python -m scripts.chat_sft \
    --device_batch_size=1 \
    --target_examples_per_step=2 \
    --num_iterations=200 \
    --eval_every=100 \
    --model_tag="$MODEL_TAG" 2>&1 | awk '/^step [0-9]/ { n=substr($2,1,5)+0; if(n%50==0) print; next } {print}'

END=$(date +%s)
echo ""
echo "âœ… SFT complete! ($((END-START))s)"
echo ""

# =============================================================================
# STAGE 4: RL
# =============================================================================
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“"
echo "â”ƒ  ğŸ¯ STAGE 4: Reinforcement Learning            â”ƒ"
echo "â”ƒ  Learning to solve maths problems              â”ƒ"
echo "â”£â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”«"
echo "â”ƒ  Steps: 30      |  Expected: ~11 min           â”ƒ"
echo "â”—â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”›"
echo ""
START=$(date +%s)

python -m scripts.chat_rl \
    --model_tag="$MODEL_TAG" \
    --device_batch_size=1 \
    --examples_per_step=4 \
    --num_samples=4 \
    --max_new_tokens=128 \
    --eval_every=30 \
    --save_every=30 \
    --num_epochs=1 2>&1 | head -600

END=$(date +%s)
echo ""
echo "âœ… RL training complete! ($((END-START))s)"
echo ""

# =============================================================================
# Summary
# =============================================================================
echo ""
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo "  ğŸ‰ PIPELINE COMPLETE!"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo ""
echo "  ğŸ“ Your checkpoints:"
echo "     Base: $NANOCHAT_BASE_DIR/base_checkpoints/$MODEL_TAG/"
echo "     Mid:  $NANOCHAT_BASE_DIR/mid_checkpoints/$MODEL_TAG/"
echo "     SFT:  $NANOCHAT_BASE_DIR/chatsft_checkpoints/$MODEL_TAG/"
echo "     RL:   $NANOCHAT_BASE_DIR/chatrl_checkpoints/$MODEL_TAG/"
echo ""
echo "  ğŸ’¬ Chat with your model:"
echo "     uv run python -m scripts.chat_cli --source=rl --model-tag=$MODEL_TAG"
echo ""
echo "  ğŸŒ Compare with full model:"
echo "     https://nanochat.karpathy.ai/"
echo ""
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo ""
